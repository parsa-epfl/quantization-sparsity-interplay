[INFO|trainer.py:1650] 2023-01-06 22:23:10,667 >> ***** Running training *****
[INFO|trainer.py:1651] 2023-01-06 22:23:10,667 >> ***** Debug *****
[INFO|trainer.py:1652] 2023-01-06 22:23:10,667 >>   Num examples = 42500
[INFO|trainer.py:1653] 2023-01-06 22:23:10,667 >>   Num Epochs = 3
[INFO|trainer.py:1654] 2023-01-06 22:23:10,667 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1655] 2023-01-06 22:23:10,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1656] 2023-01-06 22:23:10,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1657] 2023-01-06 22:23:10,667 >>   Total optimization steps = 1992
[INFO|trainer.py:1658] 2023-01-06 22:23:10,669 >>   Number of trainable parameters = 85806346
  0%|                                                                                                              | 0/1992 [00:00<?, ?it/s]/root/anaconda3/envs/huggingface_github_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 2.2546, 'learning_rate': 4.9748995983935746e-05, 'epoch': 0.02}                                                                    
{'loss': 2.0828, 'learning_rate': 4.949799196787149e-05, 'epoch': 0.03}                                                                     
{'loss': 1.8882, 'learning_rate': 4.924698795180723e-05, 'epoch': 0.05}                                                                     
{'loss': 1.6776, 'learning_rate': 4.8995983935742975e-05, 'epoch': 0.06}                                                                    
{'loss': 1.4541, 'learning_rate': 4.874497991967872e-05, 'epoch': 0.08}                                                                     
{'loss': 1.2905, 'learning_rate': 4.8493975903614455e-05, 'epoch': 0.09}                                                                    
{'loss': 1.1753, 'learning_rate': 4.8242971887550205e-05, 'epoch': 0.11}                                                                    
{'loss': 1.0827, 'learning_rate': 4.799196787148594e-05, 'epoch': 0.12}                                                                     
{'loss': 0.9805, 'learning_rate': 4.774096385542169e-05, 'epoch': 0.14}                                                                     
{'loss': 0.8855, 'learning_rate': 4.7489959839357435e-05, 'epoch': 0.15}                                                                    
{'loss': 0.8011, 'learning_rate': 4.723895582329317e-05, 'epoch': 0.17}                                                                     
{'loss': 0.8164, 'learning_rate': 4.698795180722892e-05, 'epoch': 0.18}                                                                     
{'loss': 0.7661, 'learning_rate': 4.673694779116466e-05, 'epoch': 0.2}                                                                      
{'loss': 0.7037, 'learning_rate': 4.64859437751004e-05, 'epoch': 0.21}                                                                      
{'loss': 0.6806, 'learning_rate': 4.6234939759036145e-05, 'epoch': 0.23}                                                                    
{'loss': 0.6372, 'learning_rate': 4.598393574297189e-05, 'epoch': 0.24}                                                                     
{'loss': 0.6163, 'learning_rate': 4.573293172690764e-05, 'epoch': 0.26}                                                                     
{'loss': 0.6201, 'learning_rate': 4.5481927710843374e-05, 'epoch': 0.27}                                                                    
{'loss': 0.6114, 'learning_rate': 4.523092369477912e-05, 'epoch': 0.29}                                                                     
{'loss': 0.6035, 'learning_rate': 4.497991967871486e-05, 'epoch': 0.3}                                                                      
{'loss': 0.5436, 'learning_rate': 4.4728915662650604e-05, 'epoch': 0.32}                                                                    
{'loss': 0.544, 'learning_rate': 4.447791164658635e-05, 'epoch': 0.33}                                                                      
{'loss': 0.4881, 'learning_rate': 4.422690763052209e-05, 'epoch': 0.35}                                                                     
{'loss': 0.5237, 'learning_rate': 4.3975903614457834e-05, 'epoch': 0.36}                                                                    
{'loss': 0.5007, 'learning_rate': 4.372489959839358e-05, 'epoch': 0.38}                                                                     
{'loss': 0.4832, 'learning_rate': 4.347389558232932e-05, 'epoch': 0.39}                                                                     
{'loss': 0.51, 'learning_rate': 4.3222891566265064e-05, 'epoch': 0.41}                                                                      
{'loss': 0.4885, 'learning_rate': 4.297188755020081e-05, 'epoch': 0.42}                                                                     
{'loss': 0.4898, 'learning_rate': 4.2720883534136544e-05, 'epoch': 0.44}                                                                    
{'loss': 0.5094, 'learning_rate': 4.2469879518072294e-05, 'epoch': 0.45}                                                                    
{'loss': 0.5075, 'learning_rate': 4.221887550200803e-05, 'epoch': 0.47}                                                                     
{'loss': 0.4419, 'learning_rate': 4.196787148594378e-05, 'epoch': 0.48}                                                                     
{'loss': 0.496, 'learning_rate': 4.1716867469879523e-05, 'epoch': 0.5}                                                                      
{'loss': 0.4212, 'learning_rate': 4.146586345381526e-05, 'epoch': 0.51}                                                                     
{'loss': 0.4545, 'learning_rate': 4.121485943775101e-05, 'epoch': 0.53}                                                                     
{'loss': 0.4566, 'learning_rate': 4.0963855421686746e-05, 'epoch': 0.54}                                                                    
{'loss': 0.4012, 'learning_rate': 4.071285140562249e-05, 'epoch': 0.56}                                                                     
{'loss': 0.3893, 'learning_rate': 4.046184738955823e-05, 'epoch': 0.57}                                                                     
{'loss': 0.4461, 'learning_rate': 4.0210843373493976e-05, 'epoch': 0.59}                                                                    
{'loss': 0.4423, 'learning_rate': 3.995983935742972e-05, 'epoch': 0.6}                                                                      
{'loss': 0.4496, 'learning_rate': 3.970883534136546e-05, 'epoch': 0.62}                                                                     
{'loss': 0.4069, 'learning_rate': 3.9457831325301206e-05, 'epoch': 0.63}                                                                    
{'loss': 0.4046, 'learning_rate': 3.920682730923695e-05, 'epoch': 0.65}                                                                     
{'loss': 0.4051, 'learning_rate': 3.895582329317269e-05, 'epoch': 0.66}                                                                     
{'loss': 0.399, 'learning_rate': 3.8704819277108436e-05, 'epoch': 0.68}                                                                     
{'loss': 0.4135, 'learning_rate': 3.845381526104418e-05, 'epoch': 0.69}                                                                     
{'loss': 0.4052, 'learning_rate': 3.820281124497992e-05, 'epoch': 0.71}                                                                     
{'loss': 0.4058, 'learning_rate': 3.7951807228915666e-05, 'epoch': 0.72}                                                                    
{'loss': 0.3809, 'learning_rate': 3.770080321285141e-05, 'epoch': 0.74}                                                                     
{'loss': 0.406, 'learning_rate': 3.744979919678715e-05, 'epoch': 0.75}                                                                      
{'loss': 0.3393, 'learning_rate': 3.7198795180722895e-05, 'epoch': 0.77}                                                                    
{'loss': 0.395, 'learning_rate': 3.694779116465863e-05, 'epoch': 0.78}                                                                      
{'loss': 0.428, 'learning_rate': 3.669678714859438e-05, 'epoch': 0.8}                                                                       
{'loss': 0.3877, 'learning_rate': 3.644578313253012e-05, 'epoch': 0.81}                                                                     
{'loss': 0.3714, 'learning_rate': 3.619477911646587e-05, 'epoch': 0.83}                                                                     
{'loss': 0.3621, 'learning_rate': 3.5943775100401605e-05, 'epoch': 0.84}                                                                    
{'loss': 0.3948, 'learning_rate': 3.569277108433735e-05, 'epoch': 0.86}                                                                     
{'loss': 0.4025, 'learning_rate': 3.54417670682731e-05, 'epoch': 0.87}                                                                      
{'loss': 0.3192, 'learning_rate': 3.5190763052208835e-05, 'epoch': 0.89}                                                                    
{'loss': 0.4145, 'learning_rate': 3.4939759036144585e-05, 'epoch': 0.9}                                                                     
{'loss': 0.3197, 'learning_rate': 3.468875502008032e-05, 'epoch': 0.92}                                                                     
{'loss': 0.3464, 'learning_rate': 3.4437751004016065e-05, 'epoch': 0.93}                                                                    
{'loss': 0.3854, 'learning_rate': 3.418674698795181e-05, 'epoch': 0.95}                                                                     
{'loss': 0.3747, 'learning_rate': 3.393574297188755e-05, 'epoch': 0.96}                                                                     
{'loss': 0.3454, 'learning_rate': 3.3684738955823294e-05, 'epoch': 0.98}                                                                    
{'loss': 0.3287, 'learning_rate': 3.343373493975904e-05, 'epoch': 0.99}                                                                     
 33%|█████████████████████████████████▎                                                                  | 664/1992 [13:27<26:18,  1.19s/it][INFO|trainer.py:2965] 2023-01-06 22:36:38,809 >> ***** Running Evaluation *****
[INFO|trainer.py:2967] 2023-01-06 22:36:38,809 >>   Num examples = 7500
[INFO|trainer.py:2970] 2023-01-06 22:36:38,809 >>   Batch size = 16
{'eval_loss': 0.13735751807689667, 'eval_accuracy': 0.9813333333333333, 'eval_runtime': 74.9184, 'eval_samples_per_second': 100.109, 'eval_steps_per_second': 6.26, 'epoch': 1.0}                                                                                                                                                                     
 33%|█████████████████████████████████▎                                                                  | 664/1992 [14:43<26:18,  1.19s/it]                              [INFO|trainer.py:2710] 2023-01-06 22:37:53,728 >> Saving model checkpoint to ./cifar10_outputs/checkpoint-664                                                               
[INFO|configuration_utils.py:447] 2023-01-06 22:37:53,729 >> Configuration saved in ./cifar10_outputs/checkpoint-664/config.json
[INFO|modeling_utils.py:1702] 2023-01-06 22:37:54,165 >> Model weights saved in ./cifar10_outputs/checkpoint-664/pytorch_model.bin
[INFO|image_processing_utils.py:200] 2023-01-06 22:37:54,166 >> Image processor saved in ./cifar10_outputs/checkpoint-664/preprocessor_config.json
[INFO|image_processing_utils.py:200] 2023-01-06 22:37:55,318 >> Image processor saved in ./cifar10_outputs/preprocessor_config.json
/root/anaconda3/envs/huggingface_github_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
{'loss': 0.3163, 'learning_rate': 3.318273092369478e-05, 'epoch': 1.01}                                                                     
{'loss': 0.3473, 'learning_rate': 3.2931726907630524e-05, 'epoch': 1.02}                                                                    
{'loss': 0.3449, 'learning_rate': 3.268072289156627e-05, 'epoch': 1.04}                                                                     
{'loss': 0.3646, 'learning_rate': 3.242971887550201e-05, 'epoch': 1.05}                                                                     
{'loss': 0.328, 'learning_rate': 3.2178714859437754e-05, 'epoch': 1.07}                                                                     
{'loss': 0.3194, 'learning_rate': 3.192771084337349e-05, 'epoch': 1.08}                                                                     
{'loss': 0.3443, 'learning_rate': 3.167670682730924e-05, 'epoch': 1.1}                                                                      
{'loss': 0.3589, 'learning_rate': 3.1425702811244984e-05, 'epoch': 1.11}                                                                    
{'loss': 0.3422, 'learning_rate': 3.117469879518072e-05, 'epoch': 1.13}                                                                     
{'loss': 0.2876, 'learning_rate': 3.092369477911647e-05, 'epoch': 1.14}                                                                     
{'loss': 0.3326, 'learning_rate': 3.067269076305221e-05, 'epoch': 1.16}                                                                     
{'loss': 0.296, 'learning_rate': 3.0421686746987953e-05, 'epoch': 1.17}                                                                     
{'loss': 0.3355, 'learning_rate': 3.0170682730923693e-05, 'epoch': 1.19}                                                                    
{'loss': 0.3172, 'learning_rate': 2.991967871485944e-05, 'epoch': 1.2}                                                                      
{'loss': 0.3209, 'learning_rate': 2.9668674698795183e-05, 'epoch': 1.22}                                                                    
{'loss': 0.2769, 'learning_rate': 2.9417670682730923e-05, 'epoch': 1.23}                                                                    
{'loss': 0.2686, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}                                                                     
{'loss': 0.2772, 'learning_rate': 2.891566265060241e-05, 'epoch': 1.26}                                                                     
{'loss': 0.3533, 'learning_rate': 2.8664658634538156e-05, 'epoch': 1.28}                                                                    
{'loss': 0.2737, 'learning_rate': 2.8413654618473896e-05, 'epoch': 1.3}                                                                     
{'loss': 0.2952, 'learning_rate': 2.816265060240964e-05, 'epoch': 1.31}                                                                     
{'loss': 0.3255, 'learning_rate': 2.791164658634538e-05, 'epoch': 1.33}                                                                     
{'loss': 0.3091, 'learning_rate': 2.7660642570281126e-05, 'epoch': 1.34}                                                                    
{'loss': 0.306, 'learning_rate': 2.7409638554216873e-05, 'epoch': 1.36}                                                                     
{'loss': 0.291, 'learning_rate': 2.7158634538152612e-05, 'epoch': 1.37}                                                                     
{'loss': 0.298, 'learning_rate': 2.6907630522088356e-05, 'epoch': 1.39}                                                                     
{'loss': 0.2811, 'learning_rate': 2.6656626506024096e-05, 'epoch': 1.4}                                                                     
{'loss': 0.3168, 'learning_rate': 2.6405622489959842e-05, 'epoch': 1.42}                                                                    
{'loss': 0.3199, 'learning_rate': 2.6154618473895582e-05, 'epoch': 1.43}                                                                    
{'loss': 0.2934, 'learning_rate': 2.5903614457831325e-05, 'epoch': 1.45}                                                                    
{'loss': 0.2694, 'learning_rate': 2.5652610441767072e-05, 'epoch': 1.46}                                                                    
{'loss': 0.2959, 'learning_rate': 2.5401606425702812e-05, 'epoch': 1.48}                                                                    
{'loss': 0.2489, 'learning_rate': 2.515060240963856e-05, 'epoch': 1.49}                                                                     
{'loss': 0.2786, 'learning_rate': 2.48995983935743e-05, 'epoch': 1.51}                                                                      
{'loss': 0.2741, 'learning_rate': 2.4648594377510042e-05, 'epoch': 1.52}                                                                    
{'loss': 0.3002, 'learning_rate': 2.4397590361445785e-05, 'epoch': 1.54}                                                                    
{'loss': 0.3077, 'learning_rate': 2.4146586345381528e-05, 'epoch': 1.55}                                                                    
{'loss': 0.2975, 'learning_rate': 2.389558232931727e-05, 'epoch': 1.57}                                                                     
{'loss': 0.2935, 'learning_rate': 2.364457831325301e-05, 'epoch': 1.58}                                                                     
{'loss': 0.3045, 'learning_rate': 2.3393574297188755e-05, 'epoch': 1.6}                                                                     
{'loss': 0.3084, 'learning_rate': 2.3142570281124498e-05, 'epoch': 1.61}                                                                    
{'loss': 0.3011, 'learning_rate': 2.289156626506024e-05, 'epoch': 1.63}                                                                     
{'loss': 0.2993, 'learning_rate': 2.2640562248995988e-05, 'epoch': 1.64}                                                                    
{'loss': 0.247, 'learning_rate': 2.2389558232931728e-05, 'epoch': 1.66}                                                                     
{'loss': 0.3098, 'learning_rate': 2.213855421686747e-05, 'epoch': 1.67}                                                                     
{'loss': 0.262, 'learning_rate': 2.1887550200803214e-05, 'epoch': 1.69}                                                                     
{'loss': 0.2827, 'learning_rate': 2.1636546184738958e-05, 'epoch': 1.7}                                                                     
{'loss': 0.2969, 'learning_rate': 2.13855421686747e-05, 'epoch': 1.72}                                                                      
{'loss': 0.2828, 'learning_rate': 2.113453815261044e-05, 'epoch': 1.73}                                                                     
{'loss': 0.28, 'learning_rate': 2.0883534136546184e-05, 'epoch': 1.75}                                                                      
{'loss': 0.2586, 'learning_rate': 2.063253012048193e-05, 'epoch': 1.76}                                                                     
{'loss': 0.2887, 'learning_rate': 2.0381526104417674e-05, 'epoch': 1.78}                                                                    
{'loss': 0.2788, 'learning_rate': 2.0130522088353414e-05, 'epoch': 1.79}                                                                    
{'loss': 0.2496, 'learning_rate': 1.9879518072289157e-05, 'epoch': 1.81}                                                                    
{'loss': 0.2239, 'learning_rate': 1.96285140562249e-05, 'epoch': 1.82}                                                                      
{'loss': 0.2477, 'learning_rate': 1.9377510040160644e-05, 'epoch': 1.84}                                                                    
{'loss': 0.3117, 'learning_rate': 1.9126506024096387e-05, 'epoch': 1.85}                                                                    
{'loss': 0.2605, 'learning_rate': 1.8875502008032127e-05, 'epoch': 1.87}                                                                    
 63%|█████████████████████████████████████████████████████████████▉                                     | 1247/1992 [26:52 63%|██████████████████████████████████████████████████████████████                                     | 1248/1992 [26:53 63%|██████████████████████████████████████████████████████████████                                     | 1249/1992 [26:55 63%|██████████████████████████████████████████████████████████████                                     | 1250/1992 [26:56                                                                                                                          {'loss': 0.2684, 'learning_rate': 1.8624497991967873e-05, 'epoch': 1.88}
 63%|██████████████████████████████████████████████████████████████                                     | 1250/1992 [26:56 63%|██████████████████████████████████████████████████████████████▏                                    | 1251/1992 [26:57 63%|██████████████████████████████████████████████████████████████▏                                    | 1252/1992 [26:58 63%|██████████████████████████████████████████████████████████████▎                                    | 1253/1992 [26:59 63%|██████████████████████████████████████████████████████████████▎                                    | 1254/1992 [27:01 63%|██████████████████████████████████████████████████████████████▎                                    | 1255/1992 [27:02 63%|██████████████████████████████████████████████████████████████▍                                    | 1256/1992 [27:03 63%|██████████████████████████████████████████████████████████████▍                                    | 1257/1992 [27:04 63%|██████████████████████████████████████████████████████████████▌                                    | 1258/1992 [27:05 63%|██████████████████████████████████████████████████████████████▌                                    | 1259/1992 [27:06 63%|██████████████████████████████████████████████████████████████▌                                    | 1260/1992 [27:08                                                                                                                          {'loss': 0.2434, 'learning_rate': 1.8373493975903617e-05, 'epoch': 1.9}
 63%|██████████████████████████████████████████████████████████████▌                                    | 1260/1992 [27:08 63%|██████████████████████████████████████████████████████████████▋                                    | 1261/1992 [27:09 63%|██████████████████████████████████████████████████████████████▋                                    | 1262/1992 [27:10 63%|██████████████████████████████████████████████████████████████▊                                    | 1263/1992 [27:11 63%|██████████████████████████████████████████████████████████████▊                                    | 1264/1992 [27:12 64%|██████████████████████████████████████████████████████████████▊                                    | 1265/1992 [27:13 64%|██████████████████████████████████████████████████████████████▉                                    | 1266/1992 [27:15 64%|██████████████████████████████████████████████████████████████▉                                    | 1267/1992 [27:16 64%|███████████████████████████████████████████████████████████████                                    | 1268/1992 [27:17 64%|███████████████████████████████████████████████████████████████                                    | 1269/1992 [27:18 64%|███████████████████████████████████████████████████████████████                                    | 1270/1992 [27:20                                                                                                                          {'loss': 0.3124, 'learning_rate': 1.812248995983936e-05, 'epoch': 1.91}
 64%|███████████████████████████████████████████████████████████████                                    | 1270/1992 [27:20 64%|███████████████████████████████████████████████████████████████▏                                   | 1271/1992 [27:21 64%|███████████████████████████████████████████████████████████████▏                                   | 1272/1992 [27:22 64%|███████████████████████████████████████████████████████████████▎                                   | 1273/1992 [27:23 64%|███████████████████████████████████████████████████████████████▎                                   | 1274/1992 [27:24 64%|███████████████████████████████████████████████████████████████▎                                   | 1275/1992 [27:25 64%|███████████████████████████████████████████████████████████████▍                                   | 1276/1992 [27:27 64%|███████████████████████████████████████████████████████████████▍                                   | 1277/1992 [27:28 64%|███████████████████████████████████████████████████████████████▌                                   | 1278/1992 [27:29 64%|███████████████████████████████████████████████████████████████▌                                   | 1279/1992 [27:30 64%|███████████████████████████████████████████████████████████████▌                                   | 1280/1992 [27:31                                                                                                                          {'loss': 0.3022, 'learning_rate': 1.78714859437751e-05, 'epoch': 1.93}
 64%|███████████████████████████████████████████████████████████████▌                                   | 1280/1992 [27:31 64%|███████████████████████████████████████████████████████████████▋                                   | 1281/1992 [27:33 64%|███████████████████████████████████████████████████████████████▋                                   | 1282/1992 [27:34 64%|███████████████████████████████████████████████████████████████▊                                   | 1283/1992 [27:35 64%|███████████████████████████████████████████████████████████████▊                                   | 1284/1992 [27:36 65%|███████████████████████████████████████████████████████████████▊                                   | 1285/1992 [27:37 65%|███████████████████████████████████████████████████████████████▉                                   | 1286/1992 [27:38 65%|███████████████████████████████████████████████████████████████▉                                   | 1287/1992 [27:40 65%|████████████████████████████████████████████████████████████████                                   | 1288/1992 [27:41 65%|████████████████████████████████████████████████████████████████                                   | 1289/1992 [27:42 65%|████████████████████████████████████████████████████████████████                                   | 1290/1992 [27:43                                                                                                                          {'loss': 0.2236, 'learning_rate': 1.7620481927710843e-05, 'epoch': 1.94}
 65%|████████████████████████████████████████████████████████████████                                   | 1290/1992 [27:43 65%|████████████████████████████████████████████████████████████████▏                                  | 1291/1992 [27:45 65%|████████████████████████████████████████████████████████████████▏                                  | 1292/1992 [27:46 65%|████████████████████████████████████████████████████████████████▎                                  | 1293/1992 [27:47 65%|████████████████████████████████████████████████████████████████▎                                  | 1294/1992 [27:48 65%|████████████████████████████████████████████████████████████████▎                                  | 1295/1992 [27:49 65%|████████████████████████████████████████████████████████████████▍                                  | 1296/1992 [27:50 65%|████████████████████████████████████████████████████████████████▍                                  | 1297/1992 [27:52 65%|████████████████████████████████████████████████████████████████▌                                  | 1298/1992 [27:53 65%|████████████████████████████████████████████████████████████████▌                                  | 1299/1992 [27:54 65%|████████████████████████████████████████████████████████████████▌                                  | 1300/1992 [27:55                                                                                                                          {'loss': 0.3144, 'learning_rate': 1.7369477911646586e-05, 'epoch': 1.96}
 65%|████████████████████████████████████████████████████████████████▌                                  | 1300/1992 [27:55 65%|████████████████████████████████████████████████████████████████▋                                  | 1301/1992 [27:56 65%|████████████████████████████████████████████████████████████████▋                                  | 1302/1992 [27:58 65%|████████████████████████████████████████████████████████████████▊                                  | 1303/1992 [27:59 65%|████████████████████████████████████████████████████████████████▊                                  | 1304/1992 [28:00 66%|████████████████████████████████████████████████████████████████▊                                  | 1305/1992 [28:01 66%|████████████████████████████████████████████████████████████████▉                                  | 1306/1992 [28:02 66%|████████████████████████████████████████████████████████████████▉                                  | 1307/1992 [28:04 66%|█████████████████████████████████████████████████████████████████                                  | 1308/1992 [28:05 66%|█████████████████████████████████████████████████████████████████                                  | 1309/1992 [28:06 66%|█████████████████████████████████████████████████████████████████                                  | 1310/1992 [28:07                                                                                                                          {'loss': 0.2287, 'learning_rate': 1.711847389558233e-05, 'epoch': 1.97}
 66%|█████████████████████████████████████████████████████████████████                                  | 1310/1992 [28:07 66%|█████████████████████████████████████████████████████████████████▏                                 | 1311/1992 [28:09 66%|█████████████████████████████████████████████████████████████████▏                                 | 1312/1992 [28:10 66%|█████████████████████████████████████████████████████████████████▎                                 | 1313/1992 [28:11 66%|█████████████████████████████████████████████████████████████████▎                                 | 1314/1992 [28:12 66%|█████████████████████████████████████████████████████████████████▎                                 | 1315/1992 [28:13 66%|█████████████████████████████████████████████████████████████████▍                                 | 1316/1992 [28:14 66%|█████████████████████████████████████████████████████████████████▍                                 | 1317/1992 [28:16 66%|█████████████████████████████████████████████████████████████████▌                                 | 1318/1992 [28:17 66%|█████████████████████████████████████████████████████████████████▌                                 | 1319/1992 [28:18 66%|█████████████████████████████████████████████████████████████████▌                                 | 1320/1992 [28:19                                                                                                                          {'loss': 0.2553, 'learning_rate': 1.6867469879518073e-05, 'epoch': 1.99}
 66%|█████████████████████████████████████████████████████████████████▌                                 | 1320/1992 [28:19 66%|█████████████████████████████████████████████████████████████████▋                                 | 1321/1992 [28:20 66%|█████████████████████████████████████████████████████████████████▋                                 | 1322/1992 [28:21 66%|█████████████████████████████████████████████████████████████████▊                                 | 1323/1992 [28:23 66%|█████████████████████████████████████████████████████████████████▊                                 | 1324/1992 [28:24 67%|█████████████████████████████████████████████████████████████████▊                                 | 1325/1992 [28:25 67%|█████████████████████████████████████████████████████████████████▉                                 | 1326/1992 [28:26 67%|█████████████████████████████████████████████████████████████████▉                                 | 1327/1992 [28:27 67%|██████████████████████████████████████████████████████████████████                                 | 1328/1992 [28:29<13:12,  1.19s/it][INFO|trainer.py:2965] 2023-01-06 22:51:40,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2967] 2023-01-06 22:51:40,061 >>   Num examples = 7500
[INFO|trainer.py:2970] 2023-01-06 22:51:40,061 >>   Batch size = 16
{'eval_loss': 0.08155153691768646, 'eval_accuracy': 0.9873333333333333, 'eval_runtime': 75.6894, 'eval_samples_per_second': 99.089, 'eval_steps_per_second': 6.196, 'epoch': 2.0}   
...
{'loss': 0.2284, 'learning_rate': 1.4859437751004016e-05, 'epoch': 2.11}                                                                    
{'loss': 0.2401, 'learning_rate': 1.460843373493976e-05, 'epoch': 2.12}                                                                     
{'loss': 0.2164, 'learning_rate': 1.4357429718875504e-05, 'epoch': 2.14}                                                                    
{'loss': 0.209, 'learning_rate': 1.4106425702811245e-05, 'epoch': 2.15}                                                                     
{'loss': 0.2322, 'learning_rate': 1.3855421686746989e-05, 'epoch': 2.17}                                                                    
{'loss': 0.2355, 'learning_rate': 1.3604417670682732e-05, 'epoch': 2.18}                                                                    
{'loss': 0.2447, 'learning_rate': 1.3353413654618473e-05, 'epoch': 2.2}                                                                     
{'loss': 0.24, 'learning_rate': 1.3102409638554217e-05, 'epoch': 2.21}                                                                      
{'loss': 0.2242, 'learning_rate': 1.285140562248996e-05, 'epoch': 2.23}                                                                     
{'loss': 0.2488, 'learning_rate': 1.2600401606425705e-05, 'epoch': 2.24}                                                                    
{'loss': 0.2381, 'learning_rate': 1.2349397590361447e-05, 'epoch': 2.26}                                                                    
{'loss': 0.2763, 'learning_rate': 1.209839357429719e-05, 'epoch': 2.27}                                                                     
{'loss': 0.2095, 'learning_rate': 1.1847389558232933e-05, 'epoch': 2.29}                                                                    
{'loss': 0.2497, 'learning_rate': 1.1596385542168675e-05, 'epoch': 2.3}                                                                     
{'loss': 0.2616, 'learning_rate': 1.1345381526104418e-05, 'epoch': 2.32}                                                                    
{'loss': 0.2407, 'learning_rate': 1.1094377510040161e-05, 'epoch': 2.33}                                                                    
{'loss': 0.2438, 'learning_rate': 1.0843373493975904e-05, 'epoch': 2.35}                                                                    
{'loss': 0.2434, 'learning_rate': 1.0592369477911648e-05, 'epoch': 2.36}                                                                    
{'loss': 0.2448, 'learning_rate': 1.034136546184739e-05, 'epoch': 2.38}                                                                     
{'loss': 0.2581, 'learning_rate': 1.0090361445783134e-05, 'epoch': 2.39}                                                                    
{'loss': 0.2433, 'learning_rate': 9.839357429718876e-06, 'epoch': 2.41}                                                                     
{'loss': 0.2902, 'learning_rate': 9.588353413654619e-06, 'epoch': 2.42}                                                                     
{'loss': 0.2577, 'learning_rate': 9.33734939759036e-06, 'epoch': 2.44}                                                                      
{'loss': 0.2732, 'learning_rate': 9.086345381526106e-06, 'epoch': 2.45}                                                                     
{'loss': 0.2436, 'learning_rate': 8.835341365461847e-06, 'epoch': 2.47}                                                                     
{'loss': 0.1886, 'learning_rate': 8.58433734939759e-06, 'epoch': 2.48}                                                                      
{'loss': 0.2538, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}                                                                      
{'loss': 0.2312, 'learning_rate': 8.082329317269077e-06, 'epoch': 2.51}                                                                     
{'loss': 0.2069, 'learning_rate': 7.83132530120482e-06, 'epoch': 2.53}                                                                      
{'loss': 0.2278, 'learning_rate': 7.580321285140563e-06, 'epoch': 2.54}                                                                     
{'loss': 0.2237, 'learning_rate': 7.329317269076305e-06, 'epoch': 2.56}                                                                     
{'loss': 0.2246, 'learning_rate': 7.078313253012049e-06, 'epoch': 2.58}                                                                     
{'loss': 0.279, 'learning_rate': 6.827309236947792e-06, 'epoch': 2.59}                                                                      
{'loss': 0.203, 'learning_rate': 6.576305220883534e-06, 'epoch': 2.61}                                                                      
{'loss': 0.2505, 'learning_rate': 6.325301204819277e-06, 'epoch': 2.62}                                                                     
{'loss': 0.2158, 'learning_rate': 6.074297188755021e-06, 'epoch': 2.64}                                                                     
{'loss': 0.1723, 'learning_rate': 5.823293172690764e-06, 'epoch': 2.65}                                                                     
{'loss': 0.2485, 'learning_rate': 5.572289156626506e-06, 'epoch': 2.67}                                                                     
{'loss': 0.2455, 'learning_rate': 5.3212851405622495e-06, 'epoch': 2.68}                                                                    
{'loss': 0.249, 'learning_rate': 5.070281124497992e-06, 'epoch': 2.7}                                                                       
{'loss': 0.2625, 'learning_rate': 4.819277108433735e-06, 'epoch': 2.71}                                                                     
{'loss': 0.239, 'learning_rate': 4.568273092369478e-06, 'epoch': 2.73}                                                                      
{'loss': 0.2197, 'learning_rate': 4.317269076305222e-06, 'epoch': 2.74}                                                                     
{'loss': 0.2423, 'learning_rate': 4.066265060240964e-06, 'epoch': 2.76}                                                                     
{'loss': 0.2155, 'learning_rate': 3.8152610441767074e-06, 'epoch': 2.77}                                                                    
{'loss': 0.2126, 'learning_rate': 3.56425702811245e-06, 'epoch': 2.79}                                                                      
{'loss': 0.2523, 'learning_rate': 3.313253012048193e-06, 'epoch': 2.8}                                                                      
{'loss': 0.234, 'learning_rate': 3.062248995983936e-06, 'epoch': 2.82}                                                                      
{'loss': 0.1898, 'learning_rate': 2.811244979919679e-06, 'epoch': 2.83}                                                                     
{'loss': 0.2038, 'learning_rate': 2.5602409638554217e-06, 'epoch': 2.85}                                                                    
{'loss': 0.2188, 'learning_rate': 2.3092369477911645e-06, 'epoch': 2.86}                                                                    
{'loss': 0.1889, 'learning_rate': 2.0582329317269078e-06, 'epoch': 2.88}                                                                    
{'loss': 0.2406, 'learning_rate': 1.8072289156626506e-06, 'epoch': 2.89}                                                                    
{'loss': 0.2678, 'learning_rate': 1.5562248995983937e-06, 'epoch': 2.91}                                                                    
{'loss': 0.2216, 'learning_rate': 1.3052208835341365e-06, 'epoch': 2.92}                                                                    
{'loss': 0.2125, 'learning_rate': 1.0542168674698796e-06, 'epoch': 2.94}                                                                    
{'loss': 0.2524, 'learning_rate': 8.032128514056225e-07, 'epoch': 2.95}                                                                     
{'loss': 0.2255, 'learning_rate': 5.522088353413655e-07, 'epoch': 2.97}                                                                     
{'loss': 0.238, 'learning_rate': 3.0120481927710845e-07, 'epoch': 2.98}                                                                     
{'loss': 0.2166, 'learning_rate': 5.0200803212851406e-08, 'epoch': 3.0}                                                                     
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1992/1992 [43:40<00:00,  1.19s/it][INFO|trainer.py:2965] 2023-01-06 23:06:51,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2967] 2023-01-06 23:06:51,033 >>   Num examples = 7500
[INFO|trainer.py:2970] 2023-01-06 23:06:51,033 >>   Batch size = 16
{'eval_loss': 0.07169705629348755, 'eval_accuracy': 0.9868, 'eval_runtime': 74.7252, 'eval_samples_per_second': 100.368, 'eval_steps_per_second': 6.276, 'epoch': 3.0}                                                                                                                  
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1992/1992 [44:55<00:00,  1.19s/it[INFO|trainer.py:2710] 2023-01-06 23:08:05,759 >> Saving model checkpoint to ./cifar10_outputs/checkpoint-1992                               
[INFO|configuration_utils.py:447] 2023-01-06 23:08:05,760 >> Configuration saved in ./cifar10_outputs/checkpoint-1992/config.json
[INFO|modeling_utils.py:1702] 2023-01-06 23:08:06,202 >> Model weights saved in ./cifar10_outputs/checkpoint-1992/pytorch_model.bin
[INFO|image_processing_utils.py:200] 2023-01-06 23:08:06,203 >> Image processor saved in ./cifar10_outputs/checkpoint-1992/preprocessor_config.json
[INFO|image_processing_utils.py:200] 2023-01-06 23:08:09,390 >> Image processor saved in ./cifar10_outputs/preprocessor_config.json
[INFO|trainer.py:1902] 2023-01-06 23:08:34,202 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2026] 2023-01-06 23:08:34,203 >> Loading best model from ./cifar10_outputs/checkpoint-1992 (score: 0.07169705629348755).
{'train_runtime': 2723.9471, 'train_samples_per_second': 46.807, 'train_steps_per_second': 0.731, 'train_loss': 0.38394166544139624, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1992/1992 [45:23<00:00,  1.37s/it]
[INFO|trainer.py:2710] 2023-01-06 23:08:34,618 >> Saving model checkpoint to ./cifar10_outputs/
[INFO|configuration_utils.py:447] 2023-01-06 23:08:34,620 >> Configuration saved in ./cifar10_outputs/config.json
[INFO|modeling_utils.py:1702] 2023-01-06 23:08:35,051 >> Model weights saved in ./cifar10_outputs/pytorch_model.bin
[INFO|image_processing_utils.py:200] 2023-01-06 23:08:35,052 >> Image processor saved in ./cifar10_outputs/preprocessor_config.json
[INFO|trainer.py:2710] 2023-01-06 23:08:35,053 >> Saving model checkpoint to ./cifar10_outputs/
[INFO|configuration_utils.py:447] 2023-01-06 23:08:35,054 >> Configuration saved in ./cifar10_outputs/config.json
[INFO|modeling_utils.py:1702] 2023-01-06 23:08:35,585 >> Model weights saved in ./cifar10_outputs/pytorch_model.bin
[INFO|image_processing_utils.py:200] 2023-01-06 23:08:35,585 >> Image processor saved in ./cifar10_outputs/preprocessor_config.json
Several commits (2) will be pushed upstream.
01/06/2023 23:08:54 - WARNING - huggingface_hub.repository - Several commits (2) will be pushed upstream.
The progress bars may be unreliable.
01/06/2023 23:08:54 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.
Upload file pytorch_model.bin:  98%|███████████████████████remote: Scanning LFS files for validity, may be slow...
remote: LFS file scan complete.
To https://user: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@huggingface.co/simlaharma/vit-base-cifar10
   d24a6f1..fb92d02  main -> main

01/06/2023 23:10:26 - WARNING - huggingface_hub.repository - remote: Scanning LFS files for validity, may be slow...
remote: LFS file scan complete.
To https://user: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@huggingface.co/simlaharma/vit-base-cifar10
   d24a6f1..fb92d02  main -> main

Upload file pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████| 327M/327M [01Upload file pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████| 327M/327M [01Upload file pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████| 327M/327M [01:07<00:00, 5.12MB/s]
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.3839
  train_runtime            = 0:45:23.94
  train_samples_per_second =     46.807
  train_steps_per_second   =      0.731
[INFO|trainer.py:2965] 2023-01-06 23:10:30,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2967] 2023-01-06 23:10:30,004 >>   Num examples = 7500
[INFO|trainer.py:2970] 2023-01-06 23:10:30,004 >>   Batch size = 16
/root/anaconda3/envs/huggingface_github_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
100%|█████████████████████| 469/469 [01:15<00:00,  6.25it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.9879
  eval_loss               =     0.0702
  eval_runtime            = 0:01:15.20
  eval_samples_per_second =     99.725
  eval_steps_per_second   =      6.236
[INFO|trainer.py:2710] 2023-01-06 23:11:45,211 >> Saving model checkpoint to ./cifar10_outputs/
[INFO|configuration_utils.py:447] 2023-01-06 23:11:45,211 >> Configuration saved in ./cifar10_outputs/config.json
[INFO|modeling_utils.py:1702] 2023-01-06 23:11:45,626 >> Model weights saved in ./cifar10_outputs/pytorch_model.bin
[INFO|image_processing_utils.py:200] 2023-01-06 23:11:45,627 >> Image processor saved in ./cifar10_outputs/preprocessor_config.json
To https://user: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@huggingface.co/simlaharma/vit-base-cifar10
   fb92d02..788149b  main -> main

01/06/2023 23:12:20 - WARNING - huggingface_hub.repository - To https://user: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@huggingface.co/simlaharma/vit-base-cifar10
   fb92d02..788149b  main -> main